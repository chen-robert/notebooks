{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config Vars\n",
    "LSTM_SIZE = 128\n",
    "batch_size = 500\n",
    "test_batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/ptb\"\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = file.read().replace(\"\\n\", \"<eos>\").split()\n",
    "        \n",
    "        counter = collections.Counter(data)\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x: (x[1], x[0]))\n",
    "        \n",
    "        words,_ = list(zip(*sorted_counter))\n",
    "\n",
    "        return dict(zip(words, range(len(words))))\n",
    "def file2id(filename, w2id):\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = file.read().split(\"\\n\")\n",
    "        \n",
    "        data = [(sentence + \" <eos>\").split() for sentence in data]\n",
    "        \n",
    "        wordIds = [[w2id[word] for word in sentence if word in w2id] for sentence in data]\n",
    "        maxLen = len(max(wordIds, key=len))\n",
    "        \n",
    "        return wordIds, maxLen\n",
    "        \n",
    "        \n",
    "def load_data():\n",
    "    train_path = data_path + \"/ptb.train.txt\"\n",
    "    valid_path = data_path + \"/ptb.valid.txt\"\n",
    "    test_path = data_path + \"/ptb.test.txt\"\n",
    "    \n",
    "    w2id = read_data(train_path)\n",
    "    padChar = len(w2id)\n",
    "    w2id[\"<pad>\"] = padChar\n",
    "    \n",
    "    train_data, maxLen = file2id(train_path, w2id)\n",
    "    valid_data, _ = file2id(valid_path, w2id)\n",
    "    test_data, _ = file2id(test_path, w2id)\n",
    "    \n",
    "    id2w = dict(zip(w2id.values(), w2id.keys()))\n",
    "    \n",
    "    train_data = pad(maxLen, train_data, padChar)\n",
    "    valid_data = pad(maxLen, valid_data, padChar)\n",
    "    test_data = pad(maxLen, test_data, padChar)\n",
    "    \n",
    "    \n",
    "    return train_data, valid_data, test_data, id2w, maxLen\n",
    "\n",
    "def pad(maxLen, words, padChar):\n",
    "    return [sentence + (maxLen - len(sentence)) * [padChar] for sentence in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, id2w, maxLen = load_data()\n",
    "words = len(id2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "x = tf.placeholder(tf.float32, [None, maxLen, 1])\n",
    "\n",
    "flat_x = tf.unstack(x, maxLen, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = rnn.BasicLSTMCell(LSTM_SIZE, forget_bias=1)\n",
    "outputs, _ = rnn.static_rnn(lstmCell, flat_x, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_var(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "def bias(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "W_fc = tf.Variable(weight_var([LSTM_SIZE, words]))\n",
    "b_fc = tf.Variable(bias([words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = tf.zeros([1])\n",
    "\n",
    "\n",
    "correct_list = []\n",
    "predictions = []\n",
    "for i in range(len(outputs) - 1):\n",
    "    output = outputs[i]\n",
    "    \n",
    "    pred = tf.matmul(output, W_fc) + b_fc\n",
    "    \n",
    "    predictions.append(tf.nn.softmax(pred))\n",
    "    \n",
    "    choice = tf.argmax(predictions[i], 1), \n",
    "    correct = tf.equal(choice, tf.cast(flat_x[i+1], tf.int64))\n",
    "    \n",
    "    correct_list.append(correct)\n",
    "    \n",
    "    \n",
    "    labels = tf.reshape(flat_x[i+1], [-1])\n",
    "    labels = tf.cast(labels, tf.int32)\n",
    "    \n",
    "    cross_entro = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=labels))\n",
    "    \n",
    "    loss = loss + cross_entro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accur = tf.reduce_mean(tf.cast(tf.stack(correct_list), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(1e-4)\n",
    "\n",
    "gradients, params = zip(*opt.compute_gradients(loss))\n",
    "clipped_grad, _ = tf.clip_by_global_norm(gradients, 5)\n",
    "\n",
    "train = opt.apply_gradients(zip(clipped_grad, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(dataset, batch_size = 1):\n",
    "    indexs = np.random.randint(0, len(dataset), [batch_size])\n",
    "    random_data = [dataset[i] for i in indexs]\n",
    "    \n",
    "    return np.array(random_data, dtype=\"float32\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 00000 | Training loss 756, accur 0.000000 | Test loss 756\n",
      "Iter 00100 | Training loss 718, accur 0.000000 | Test loss 718\n",
      "Iter 00200 | Training loss 679, accur 0.001644 | Test loss 679\n",
      "Iter 00300 | Training loss 620, accur 0.007048 | Test loss 619\n",
      "Iter 00400 | Training loss 541, accur 0.006685 | Test loss 541\n"
     ]
    }
   ],
   "source": [
    "#Training!\n",
    "for iter in range(500):\n",
    "    if iter % 100 == 0:\n",
    "        train_loss = loss.eval(feed_dict={x: generate_data(train_data, batch_size).reshape(batch_size, maxLen, 1)})\n",
    "        test_loss = loss.eval(feed_dict={x: generate_data(test_data, len(test_data)).reshape(len(test_data), maxLen, 1)})\n",
    "        \n",
    "        train_accur = accur.eval(feed_dict={x: generate_data(train_data, test_batch_size).reshape(test_batch_size, maxLen, 1)})\n",
    "        \n",
    "        print(\"Iter %05d | Training loss %d, accur %f | Test loss %d\"%(iter, train_loss, train_accur, test_loss))\n",
    "    train.run(feed_dict={x: generate_data(train_data, batch_size).reshape(batch_size, maxLen, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tax-loss buddy promising badly satisfied cananea seized abc N midland governor theme n't programming latest it arias a cambria dax stateswest hard-disk up widespread winter wilfred banponce poles subsidy overhaul <unk> kitchen aspect affiliated he more boomers component investors above wright dreams corner sentencing manitoba convex entry group jim jailed impression spent suggested air depends useful enabling glasnost intensify parents david withdrawal reality intermediate broken N gaubert carriers in reading solutions organization clark music sustain others jobs and association to cohen popularity "
     ]
    }
   ],
   "source": [
    "#Generate a sentence! With a really dumb method :(\n",
    "gen_sent = np.array([np.random.randint(len(id2w))] * maxLen)\n",
    "print(id2w[gen_sent[0]], end=\" \")\n",
    "for i in range(1, maxLen-1):\n",
    "    curr = predictions[i].eval(feed_dict={x: gen_sent.reshape(1, maxLen, 1)})[0]\n",
    "    \n",
    "    choice = np.random.choice(range(len(curr)), p=curr)\n",
    "    \n",
    "    gen_sent[i] = choice\n",
    "    \n",
    "    print(id2w[choice], end=\" \")\n",
    "    \n",
    "    if id2w[choice] == \"<eos>\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
