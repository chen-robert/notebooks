{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-83231f068ae1>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/robert/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/robert/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/robert/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/robert/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/robert/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_var(shape, dropout=True, stddev=0.05):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    \n",
    "    return var\n",
    "def bias(shape):\n",
    "    return tf.Variable(tf.constant(0.01, shape=shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "D_w1 = weight_var([784, 128])\n",
    "D_b1 = bias([128])\n",
    "\n",
    "D_w2 = weight_var([128, 1])\n",
    "D_b2 = bias([1])\n",
    "\n",
    "def discrim(x):\n",
    "    D_out1 = tf.nn.relu(tf.matmul(x, D_w1) + D_b1)\n",
    "    D_out2 = tf.matmul(D_out1, D_w2) + D_b2\n",
    "    D_probs = tf.nn.sigmoid(D_out2)\n",
    "    \n",
    "    return D_out2, D_probs\n",
    "\n",
    "D_vars = [D_w1, D_b1, D_w2, D_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = tf.placeholder(tf.float32, [None, 100])\n",
    "\n",
    "G_w1 = weight_var([100, 128])\n",
    "G_b1 = bias([128])\n",
    "\n",
    "G_out1 = tf.nn.relu(tf.matmul(Z, G_w1) + G_b1)\n",
    "\n",
    "G_w2 = weight_var([128, 784])\n",
    "G_b2 = bias([784])\n",
    "\n",
    "G_out2 = tf.matmul(G_out1, G_w2) + G_b2\n",
    "\n",
    "G_sample = tf.nn.sigmoid(G_out2)\n",
    "G_sample_image = tf.summary.image(\"generator\", tf.reshape(G_sample, [-1, 28, 28, 1]), 3)\n",
    "\n",
    "\n",
    "G_vars = [G_w1, G_b1, G_w2, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real_logits, D_real = discrim(X)\n",
    "D_fake_logits, D_fake = discrim(G_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real_loss = tf.log(tf.clip_by_value(D_real, 1e-10, 1.0))\n",
    "D_fake_loss = tf.log(tf.clip_by_value(1. - D_fake, 1e-10, 1.0))\n",
    "\n",
    "\n",
    "\n",
    "D_loss = -tf.reduce_mean(D_real_loss + D_fake_loss)\n",
    "G_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(D_fake, 1e-10, 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_trainer = tf.train.AdamOptimizer(1e-4).minimize(D_loss, var_list=D_vars)\n",
    "G_trainer = tf.train.AdamOptimizer(1e-4).minimize(G_loss, var_list=G_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(\"tmp/logs\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(batch_size):\n",
    "    return np.random.uniform(-1, 1, [batch_size, 100])\n",
    "\n",
    "np.random.seed(0xDEADBEEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Losses: 0.61 and 1.99\n",
      "Iter 5000: Losses: 0.66 and 1.85\n",
      "Iter 10000: Losses: 0.57 and 2.20\n",
      "Iter 15000: Losses: 0.57 and 2.11\n",
      "Iter 20000: Losses: 0.52 and 2.52\n",
      "Iter 25000: Losses: 0.73 and 1.83\n",
      "Iter 30000: Losses: 0.56 and 2.16\n",
      "Iter 35000: Losses: 0.48 and 2.61\n",
      "Iter 40000: Losses: 0.48 and 2.71\n",
      "Iter 45000: Losses: 0.55 and 2.31\n",
      "Iter 50000: Losses: 0.71 and 2.12\n",
      "Iter 55000: Losses: 0.59 and 2.18\n",
      "Iter 60000: Losses: 0.51 and 2.53\n",
      "Iter 65000: Losses: 0.65 and 2.04\n",
      "Iter 70000: Losses: 1.14 and 1.23\n",
      "Iter 75000: Losses: 0.60 and 2.06\n",
      "Iter 80000: Losses: 0.54 and 2.46\n",
      "Iter 85000: Losses: 1.40 and 1.37\n",
      "Iter 90000: Losses: 0.50 and 2.41\n",
      "Iter 95000: Losses: 0.76 and 1.87\n",
      "Iter 100000: Losses: 0.70 and 1.81\n",
      "Iter 105000: Losses: 0.80 and 1.86\n",
      "Iter 110000: Losses: 0.63 and 2.12\n",
      "Iter 115000: Losses: 0.52 and 2.34\n",
      "Iter 120000: Losses: 0.62 and 2.17\n",
      "Iter 125000: Losses: 0.55 and 2.41\n",
      "Iter 130000: Losses: 0.65 and 2.38\n",
      "Iter 135000: Losses: 1.79 and 1.28\n",
      "Iter 140000: Losses: 0.58 and 2.28\n",
      "Iter 145000: Losses: 0.59 and 2.22\n",
      "Iter 150000: Losses: 0.56 and 2.56\n",
      "Iter 155000: Losses: 0.59 and 2.43\n",
      "Iter 160000: Losses: 0.78 and 2.03\n",
      "Iter 165000: Losses: 0.66 and 1.88\n",
      "Iter 170000: Losses: 0.68 and 1.87\n",
      "Iter 175000: Losses: 0.70 and 2.02\n",
      "Iter 180000: Losses: 0.59 and 2.24\n",
      "Iter 185000: Losses: 0.69 and 2.00\n",
      "Iter 190000: Losses: 0.59 and 2.17\n",
      "Iter 195000: Losses: 0.57 and 2.38\n",
      "Iter 200000: Losses: 0.52 and 2.56\n",
      "Iter 205000: Losses: 0.85 and 1.81\n",
      "Iter 210000: Losses: 0.67 and 1.92\n",
      "Iter 215000: Losses: 1.42 and 2.32\n",
      "Iter 220000: Losses: 0.75 and 2.69\n",
      "Iter 225000: Losses: 1.60 and 2.21\n",
      "Iter 230000: Losses: 0.84 and 1.96\n",
      "Iter 235000: Losses: 0.81 and 1.95\n",
      "Iter 240000: Losses: 0.81 and 1.70\n",
      "Iter 245000: Losses: 0.77 and 1.86\n",
      "Iter 250000: Losses: 0.69 and 1.95\n",
      "Iter 255000: Losses: 1.34 and 1.21\n",
      "Iter 260000: Losses: 0.77 and 1.76\n",
      "Iter 265000: Losses: 0.65 and 1.96\n",
      "Iter 270000: Losses: 0.58 and 2.07\n",
      "Iter 275000: Losses: 0.79 and 1.82\n",
      "Iter 280000: Losses: 0.66 and 1.97\n",
      "Iter 285000: Losses: 0.63 and 2.05\n",
      "Iter 290000: Losses: 0.63 and 2.15\n",
      "Iter 295000: Losses: 1.03 and 2.03\n",
      "Iter 300000: Losses: 1.16 and 1.58\n",
      "Iter 305000: Losses: 0.71 and 1.83\n",
      "Iter 310000: Losses: 0.65 and 2.29\n",
      "Iter 315000: Losses: 0.71 and 1.87\n",
      "Iter 320000: Losses: 0.73 and 1.84\n",
      "Iter 325000: Losses: 0.68 and 2.01\n",
      "Iter 330000: Losses: 0.61 and 2.09\n",
      "Iter 335000: Losses: 0.66 and 2.18\n",
      "Iter 340000: Losses: 0.54 and 3.79\n",
      "Iter 345000: Losses: 0.40 and 3.16\n",
      "Iter 350000: Losses: 0.65 and 2.08\n",
      "Iter 355000: Losses: 0.48 and 2.33\n",
      "Iter 360000: Losses: 0.73 and 1.80\n",
      "Iter 365000: Losses: 0.68 and 1.96\n",
      "Iter 370000: Losses: 0.68 and 1.97\n",
      "Iter 375000: Losses: 0.64 and 2.27\n",
      "Iter 380000: Losses: 0.68 and 1.97\n",
      "Iter 385000: Losses: 0.70 and 1.82\n",
      "Iter 390000: Losses: 0.83 and 1.59\n",
      "Iter 395000: Losses: 0.69 and 1.83\n",
      "Iter 400000: Losses: 0.68 and 1.89\n",
      "Iter 405000: Losses: 0.67 and 1.94\n",
      "Iter 410000: Losses: 0.64 and 2.10\n",
      "Iter 415000: Losses: 0.73 and 1.72\n",
      "Iter 420000: Losses: 0.84 and 1.61\n",
      "Iter 425000: Losses: 0.68 and 1.83\n",
      "Iter 430000: Losses: 0.57 and 1.94\n",
      "Iter 435000: Losses: 0.68 and 1.85\n",
      "Iter 440000: Losses: 0.75 and 1.95\n",
      "Iter 445000: Losses: 1.09 and 1.55\n",
      "Iter 450000: Losses: 0.79 and 1.64\n",
      "Iter 455000: Losses: 2.76 and 0.81\n",
      "Iter 460000: Losses: 0.75 and 1.82\n",
      "Iter 465000: Losses: 0.69 and 2.04\n",
      "Iter 470000: Losses: 0.80 and 1.62\n",
      "Iter 475000: Losses: 0.77 and 1.86\n",
      "Iter 480000: Losses: 0.84 and 1.70\n",
      "Iter 485000: Losses: 0.68 and 1.93\n",
      "Iter 490000: Losses: 0.70 and 1.98\n",
      "Iter 495000: Losses: 0.66 and 1.94\n",
      "Iter 500000: Losses: 0.69 and 1.98\n",
      "Iter 505000: Losses: 0.76 and 2.03\n",
      "Iter 510000: Losses: 0.70 and 1.87\n",
      "Iter 515000: Losses: 0.62 and 2.17\n",
      "Iter 520000: Losses: 0.77 and 1.84\n",
      "Iter 525000: Losses: 0.63 and 2.03\n",
      "Iter 530000: Losses: 0.75 and 2.01\n",
      "Iter 535000: Losses: 0.64 and 2.04\n",
      "Iter 540000: Losses: 0.78 and 1.98\n",
      "Iter 545000: Losses: 0.70 and 1.84\n",
      "Iter 550000: Losses: 0.72 and 1.78\n",
      "Iter 555000: Losses: 0.74 and 1.76\n",
      "Iter 560000: Losses: 0.63 and 2.09\n",
      "Iter 565000: Losses: 0.60 and 2.11\n",
      "Iter 570000: Losses: 0.63 and 2.16\n",
      "Iter 575000: Losses: 0.61 and 2.17\n",
      "Iter 580000: Losses: 0.69 and 2.03\n",
      "Iter 585000: Losses: 0.63 and 1.99\n",
      "Iter 590000: Losses: 0.62 and 2.26\n",
      "Iter 595000: Losses: 0.65 and 1.96\n",
      "Iter 600000: Losses: 0.66 and 2.23\n",
      "Iter 605000: Losses: 0.74 and 2.20\n",
      "Iter 610000: Losses: 0.63 and 2.13\n",
      "Iter 615000: Losses: 0.63 and 2.23\n",
      "Iter 620000: Losses: 0.63 and 2.15\n",
      "Iter 625000: Losses: 0.55 and 2.34\n",
      "Iter 630000: Losses: 0.53 and 2.46\n",
      "Iter 635000: Losses: 0.60 and 2.16\n",
      "Iter 640000: Losses: 0.55 and 2.55\n",
      "Iter 645000: Losses: 0.69 and 2.00\n",
      "Iter 650000: Losses: 0.65 and 2.01\n",
      "Iter 655000: Losses: 0.58 and 2.28\n",
      "Iter 660000: Losses: 0.72 and 2.24\n",
      "Iter 665000: Losses: 0.94 and 1.79\n",
      "Iter 670000: Losses: 0.64 and 2.05\n",
      "Iter 675000: Losses: 0.64 and 2.16\n",
      "Iter 680000: Losses: 0.63 and 2.19\n",
      "Iter 685000: Losses: 0.66 and 2.22\n",
      "Iter 690000: Losses: 0.57 and 2.33\n",
      "Iter 695000: Losses: 0.56 and 2.46\n",
      "Iter 700000: Losses: 0.56 and 2.50\n",
      "Iter 705000: Losses: 0.63 and 2.33\n",
      "Iter 710000: Losses: 0.56 and 2.33\n",
      "Iter 715000: Losses: 0.57 and 2.45\n",
      "Iter 720000: Losses: 0.92 and 1.92\n",
      "Iter 725000: Losses: 0.55 and 2.36\n",
      "Iter 730000: Losses: 0.54 and 2.58\n",
      "Iter 735000: Losses: 0.55 and 2.55\n",
      "Iter 740000: Losses: 0.56 and 2.52\n",
      "Iter 745000: Losses: 0.55 and 2.53\n",
      "Iter 750000: Losses: 0.52 and 2.57\n",
      "Iter 755000: Losses: 0.53 and 2.51\n",
      "Iter 760000: Losses: 0.56 and 2.39\n",
      "Iter 765000: Losses: 0.53 and 2.57\n",
      "Iter 770000: Losses: 0.54 and 2.63\n",
      "Iter 775000: Losses: 0.51 and 2.67\n",
      "Iter 780000: Losses: 0.51 and 2.79\n",
      "Iter 785000: Losses: 0.55 and 2.69\n",
      "Iter 790000: Losses: 0.54 and 2.71\n",
      "Iter 795000: Losses: 0.52 and 2.71\n",
      "Iter 800000: Losses: 0.53 and 2.74\n",
      "Iter 805000: Losses: 0.49 and 2.66\n",
      "Iter 810000: Losses: 0.48 and 2.80\n",
      "Iter 815000: Losses: 0.90 and 2.58\n",
      "Iter 820000: Losses: 0.54 and 2.44\n",
      "Iter 825000: Losses: 0.51 and 2.53\n",
      "Iter 830000: Losses: 0.54 and 2.73\n",
      "Iter 835000: Losses: 0.54 and 2.69\n",
      "Iter 840000: Losses: 0.49 and 2.76\n",
      "Iter 845000: Losses: 0.64 and 2.66\n",
      "Iter 850000: Losses: 0.54 and 2.60\n",
      "Iter 855000: Losses: 0.51 and 2.65\n",
      "Iter 860000: Losses: 0.52 and 2.75\n",
      "Iter 865000: Losses: 0.53 and 2.71\n",
      "Iter 870000: Losses: 0.51 and 2.84\n",
      "Iter 875000: Losses: 0.52 and 2.81\n",
      "Iter 880000: Losses: 0.47 and 2.80\n",
      "Iter 885000: Losses: 0.63 and 2.74\n",
      "Iter 890000: Losses: 0.51 and 2.80\n",
      "Iter 895000: Losses: 0.52 and 2.79\n",
      "Iter 900000: Losses: 0.48 and 2.70\n",
      "Iter 905000: Losses: 0.46 and 3.10\n",
      "Iter 910000: Losses: 0.59 and 3.18\n",
      "Iter 915000: Losses: 0.54 and 2.85\n",
      "Iter 920000: Losses: 0.53 and 2.75\n",
      "Iter 925000: Losses: 0.47 and 2.83\n",
      "Iter 930000: Losses: 0.53 and 2.88\n",
      "Iter 935000: Losses: 0.51 and 3.02\n",
      "Iter 940000: Losses: 0.42 and 2.97\n",
      "Iter 945000: Losses: 0.51 and 2.83\n",
      "Iter 950000: Losses: 0.50 and 2.73\n",
      "Iter 955000: Losses: 0.49 and 2.83\n",
      "Iter 960000: Losses: 0.45 and 2.84\n",
      "Iter 965000: Losses: 0.46 and 2.99\n",
      "Iter 970000: Losses: 0.56 and 2.78\n",
      "Iter 975000: Losses: 0.49 and 2.82\n",
      "Iter 980000: Losses: 0.50 and 2.95\n",
      "Iter 985000: Losses: 0.54 and 2.96\n",
      "Iter 990000: Losses: 0.48 and 2.98\n",
      "Iter 995000: Losses: 0.52 and 2.81\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "\n",
    "for ITER in range(1000000):\n",
    "    batch, _ = mnist.train.next_batch(BATCH_SIZE)\n",
    "    fake_batch = sample_Z(BATCH_SIZE)\n",
    "    \n",
    "    D_trainer.run(feed_dict={Z: fake_batch, X: batch})\n",
    "    G_trainer.run(feed_dict={Z: fake_batch, X: batch})\n",
    "    \n",
    "    if ITER % 5000 == 0:\n",
    "        D_train_loss = D_loss.eval(feed_dict={Z: fake_batch, X: batch})\n",
    "        G_train_loss = G_loss.eval(feed_dict={Z: fake_batch, X: batch})\n",
    "        \n",
    "        print(\"Iter %d: Losses: %.2f and %.2f\"%(ITER, D_train_loss, G_train_loss))\n",
    "        \n",
    "        summary_writer.add_summary(G_sample_image.eval(feed_dict={Z: fake_batch, X: batch}), ITER)\n",
    "        summary_writer.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
