{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-820036012056>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/robert/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/robert/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/robert/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/robert/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/robert/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_VEC_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, is_train=True, out_channel_dim=1):\n",
    "    alpha = 0.2\n",
    "    fc_size = 128\n",
    "    \n",
    "    with tf.variable_scope('generator', reuse=False if is_train==True else True):\n",
    "        fc_1 = tf.layers.dense(z, 1024)\n",
    "        batch_norm_1 = tf.layers.batch_normalization(fc_1, training=is_train)\n",
    "        lrelu_1 = tf.nn.leaky_relu(batch_norm_1, alpha)\n",
    "                \n",
    "        fc_2 = tf.layers.dense(fc_1, 7 * 7 * fc_size)    \n",
    "        \n",
    "        deconv_3 = tf.reshape(fc_2, [-1, 7, 7, fc_size])\n",
    "        batch_norm_3 = tf.layers.batch_normalization(deconv_3, training=is_train)\n",
    "        lrelu_3 = tf.nn.leaky_relu(batch_norm_3, alpha)\n",
    "        \n",
    "        deconv_4 = tf.layers.conv2d_transpose(lrelu_3, fc_size // 2, 5, 2, padding=\"SAME\")\n",
    "        batch_norm_4 = tf.layers.batch_normalization(deconv_4, training=is_train)\n",
    "        lrelu_4 = tf.nn.leaky_relu(batch_norm_4, alpha)\n",
    "        \n",
    "        deconv_5 = tf.layers.conv2d_transpose(lrelu_4, fc_size // 4, 5, 2, padding=\"SAME\")\n",
    "        batch_norm_5 = tf.layers.batch_normalization(deconv_5, training=is_train)\n",
    "        lrelu_5 = tf.nn.leaky_relu(batch_norm_5, alpha)\n",
    "        \n",
    "        print(\"Shapes:\")\n",
    "        print(lrelu_3.shape)\n",
    "        print(lrelu_4.shape)\n",
    "        print(lrelu_5.shape)\n",
    "        \n",
    "        logits = tf.layers.conv2d(lrelu_5, out_channel_dim, 5, padding=\"SAME\")\n",
    "        \n",
    "        return tf.tanh(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim(images, reuse=False, is_train=True, num_channels=1):\n",
    "    alpha = 0.2\n",
    "    \n",
    "    \n",
    "    #I have no idea why the variable naming convention suddenly changed here\n",
    "    with tf.variable_scope(\"discrim\", reuse=reuse):\n",
    "        conv1 = tf.layers.conv2d(images, 256, 5, 1, \"VALID\")\n",
    "        lrelu1 = tf.nn.leaky_relu(conv1, alpha)\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(lrelu1, 128, 5, 2, \"SAME\")\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2, training=is_train)\n",
    "        lrelu2 = tf.nn.leaky_relu(batch_norm2, alpha)\n",
    "        \n",
    "        conv3 = tf.layers.conv2d(lrelu2, 64, 5, 1, \"SAME\")\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3, training=is_train)\n",
    "        lrelu3 = tf.nn.leaky_relu(batch_norm3, alpha)\n",
    "        \n",
    "        print(\"DISCRIM\")\n",
    "        print(lrelu1.shape)\n",
    "        print(lrelu2.shape)\n",
    "        print(lrelu3.shape)\n",
    "        flat_lrelu3 = tf.reshape(lrelu3, [-1, 6**2 *64])\n",
    "        \n",
    "        fc1 = tf.layers.dense(flat_lrelu3, 512)\n",
    "        logits = tf.layers.dense(fc1, 1) \n",
    "        \n",
    "        return logits, tf.sigmoid(logits)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, num_channels=1):\n",
    "    label_smoothing = 0.9\n",
    "    \n",
    "    fake_out = generator(input_z)\n",
    "    G_sample_image = tf.summary.image(\"Deep_GAN\", fake_out)\n",
    "    \n",
    "    real_logits, real_probs = discrim(tf.reshape(input_real, [-1, 28, 28, num_channels]))\n",
    "    fake_logits, fake_probs = discrim(fake_out, True)\n",
    "    \n",
    "    D_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=label_smoothing * tf.ones_like(real_logits)))\n",
    "    D_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n",
    "    \n",
    "    D_accur_real = tf.reduce_mean(tf.round(real_probs))\n",
    "    D_accur_fake = tf.reduce_mean(tf.round(1 - fake_probs))\n",
    "    \n",
    "    D_accur = ((D_accur_real + D_accur_fake) / 2, D_accur_real, D_accur_fake)\n",
    "    \n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    \n",
    "    G_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=label_smoothing * tf.ones_like(fake_logits)))\n",
    "    \n",
    "    return D_loss, D_accur, G_loss, G_sample_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt(D_loss, G_loss, learning_rate, beta1, weight_decay=0.01):\n",
    "    t_vars = tf.trainable_variables()\n",
    "    \n",
    "    D_loss += tf.add_n([tf.nn.l2_loss(var) for var in t_vars if var.name.startswith(\"discrim\")]) * weight_decay\n",
    "    d_train = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(\n",
    "        D_loss, var_list=[var for var in t_vars if var.name.startswith(\"discrim\")])\n",
    "    \n",
    "    \n",
    "    G_loss += tf.add_n([tf.nn.l2_loss(var) for var in t_vars if var.name.startswith(\"generator\")]) * weight_decay\n",
    "    g_train = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(\n",
    "        G_loss, var_list=[var for var in t_vars if var.name.startswith(\"generator\")])\n",
    "    \n",
    "    return d_train, g_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "(?, 7, 7, 128)\n",
      "(?, 14, 14, 64)\n",
      "(?, 28, 28, 32)\n",
      "DISCRIM\n",
      "(?, 24, 24, 256)\n",
      "(?, 12, 12, 128)\n",
      "(?, 12, 12, 64)\n",
      "DISCRIM\n",
      "(?, 24, 24, 256)\n",
      "(?, 12, 12, 128)\n",
      "(?, 12, 12, 64)\n"
     ]
    }
   ],
   "source": [
    "Z = tf.placeholder(tf.float32, [None, RANDOM_VEC_SIZE])\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "D_loss, D_accur, G_loss, G_sample_image = model_loss(X, Z)\n",
    "D_trainer, G_trainer = opt(D_loss, G_loss, 2e-5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(batch_size):\n",
    "    return np.random.uniform(-1, 1, [batch_size, RANDOM_VEC_SIZE])\n",
    "\n",
    "np.random.seed(0xDEADBEEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(\"tmp/logs\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 25\n",
    "FAKE_BATCH_SIZE = 25\n",
    "\n",
    "DISCR_ITERS = 1\n",
    "GEN_ITERS = 100\n",
    "\n",
    "for ITER in range(100000):\n",
    "    batch, _ = mnist.train.next_batch(BATCH_SIZE)\n",
    "    fake_batch = sample_Z(FAKE_BATCH_SIZE)\n",
    "    \n",
    "    for _ in range(DISCR_ITERS):\n",
    "        D_trainer.run(feed_dict={Z: fake_batch, X: batch})\n",
    "    for _ in range(GEN_ITERS):\n",
    "        G_trainer.run(feed_dict={Z: fake_batch, X: batch})\n",
    "    \n",
    "    if ITER % 10 == 0:\n",
    "        if ITER % 100 == 0:\n",
    "            \n",
    "            batch, _ = mnist.train.next_batch(BATCH_SIZE)\n",
    "            fake_batch = sample_Z(FAKE_BATCH_SIZE)\n",
    "            \n",
    "            D_train_loss = D_loss.eval(feed_dict={Z: fake_batch, X: batch})\n",
    "            G_train_loss = G_loss.eval(feed_dict={Z: fake_batch, X: batch})\n",
    "            \n",
    "            D_train_accur = [part.eval(feed_dict={Z: fake_batch, X: batch}) for part in D_accur]\n",
    "\n",
    "            print(\"Iter %05d: Losses: %.2f and %.2f\"%(ITER, D_train_loss, G_train_loss))\n",
    "            print(\"Accur: %.2f%% | Real: %.2f%% | Fake: %.2f%%\" % (100 * D_train_accur[0], 100 * D_train_accur[1], 100 * D_train_accur[2]))\n",
    "        \n",
    "        summary_writer.add_summary(G_sample_image.eval(feed_dict={Z: fake_batch, X: batch}), ITER)\n",
    "        summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
